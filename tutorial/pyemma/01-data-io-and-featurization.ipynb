{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4139cd09",
   "metadata": {},
   "source": [
    "## 01. Data I/O and Featurization\n",
    "\n",
    "There are 12 Trp-Cage (TC5b) trajectories we will load and visualize.\n",
    "\n",
    "If you haven't yet downloaded these files, please follow the instructions in `../TC5b-data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "268c04e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['../TC5b-data/p16959r9c29-whole.xtc', '../TC5b-data/p16959r9c83-whole.xtc', '../TC5b-data/p16959r9c100-whole.xtc', '../TC5b-data/p16959r9c109-whole.xtc', '../TC5b-data/p16959r9c127-whole.xtc', '../TC5b-data/p16959r9c151-whole.xtc', '../TC5b-data/p16959r9c166-whole.xtc', '../TC5b-data/p16959r9c219-whole.xtc', '../TC5b-data/p16959r9c312-whole.xtc', '../TC5b-data/p16959r9c394-whole.xtc', '../TC5b-data/p16959r9c631-whole.xtc', '../TC5b-data/p16959r9c726-whole.xtc']\n",
      "Loading ../TC5b-data/p16959r9c29-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c83-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c100-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c109-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c127-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c151-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c166-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c219-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c312-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c394-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c631-whole.xtc... Processing ......Done\n",
      "Loading ../TC5b-data/p16959r9c726-whole.xtc... Processing ......Done\n"
     ]
    }
   ],
   "source": [
    "import os, glob\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import mdshare\n",
    "import pyemma\n",
    "\n",
    "# for visualization of molecular structures:\n",
    "import nglview\n",
    "import mdtraj\n",
    "from threading import Timer\n",
    "from nglview.player import TrajectoryPlayer\n",
    "\n",
    "datadir = '../TC5b-data'\n",
    "filenames = [os.path.join(datadir, s.strip()) for s in \"\"\"p16959r9c29-whole.xtc \n",
    "p16959r9c83-whole.xtc \n",
    "p16959r9c100-whole.xtc\n",
    "p16959r9c109-whole.xtc\n",
    "p16959r9c127-whole.xtc\n",
    "p16959r9c151-whole.xtc\n",
    "p16959r9c166-whole.xtc\n",
    "p16959r9c219-whole.xtc\n",
    "p16959r9c312-whole.xtc\n",
    "p16959r9c394-whole.xtc\n",
    "p16959r9c631-whole.xtc\n",
    "p16959r9c726-whole.xtc\"\"\".split('\\n')]\n",
    "print(filenames)\n",
    "\n",
    "pdbfile = os.path.join(datadir, 'xtc_atoms.gro')\n",
    "\n",
    "# Create a list of mdtraj Trajectory() objects\n",
    "trajs = []\n",
    "for fn in filenames:\n",
    "    print(f'Loading {fn}... ', end='')\n",
    "    trj = mdtraj.load(fn, top=pdbfile)\n",
    "    \n",
    "    print('Processing ...', end='')\n",
    "    # 1. they have ions in them -- let's strip these out\n",
    "    trj_protein = trj.atom_slice(trj.top.select(\"protein\"))\n",
    "    # 2. they are not centered - let's center them\n",
    "    trj_protein.center_coordinates()\n",
    "    # 3. they are not RMSD-aligned - let's center them on a frame at ~4 us\n",
    "    trj_protein.superpose(trj_protein, frame=4000, parallel=True)\n",
    "    trajs.append( trj_protein )\n",
    "    print('...Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1e212ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fff3d78c7f7e4d8c8f75bd835d22eb3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=44183)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "widget = nglview.show_mdtraj(trajs[0])\n",
    "p = TrajectoryPlayer(widget)\n",
    "widget.add_ball_and_stick()\n",
    "p.spin = True\n",
    "# def stop_spin():\n",
    "#     p.spin = False\n",
    "#    widget.close()\n",
    "# Timer(30, stop_spin).start()\n",
    "widget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58911fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "NGLWidget(max_frame=44183)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5a1164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#/usr/bin/python\n",
    "'''\n",
    "Tim Marshall\n",
    "11/07/2021\n",
    "tmchmbusiness@gmail.com\n",
    "'''\n",
    "import os, sys, glob, subprocess, time, csv, itertools, math\n",
    "import numpy as np\n",
    "import pyemma\n",
    "import h5py\n",
    "\n",
    "import mdtraj\n",
    "'''This class is designed to intake simulation data from user specified dir and process it into featurized npy files\n",
    "featurization is dependent on the function use.\n",
    "'''\n",
    "\n",
    "class organizer():\n",
    "    def __init__(self):\n",
    "        \"\"\"Initial function.\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        \"\"\"\n",
    "        self.title='Featurizer to grab data from xtc files in user specified dir'\n",
    "    def get_files(self, mydir, extensions):\n",
    "        \"\"\"Function to locate featurization relevant files from mydir. \n",
    "        \n",
    "        All files are listed under user-defined mydir.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mydir: str\n",
    "            This is path to the directories containing data in individual RUN directories\n",
    "            |>mydir\n",
    "                |>RUN0\n",
    "                    |>*.xtc\n",
    "                    |>*.gro\n",
    "                    |>.ndx\n",
    "                |>RUN1\n",
    "                    |>*.xtc\n",
    "        extensions: list(strs)\n",
    "            A list of strings containing extensions to ['trajectory', 'structure' , 'index files'] as in ['.xtc','.gro',.'ndx']\n",
    "            If you are working with P16958 gromacs style format, just leave this be. \n",
    "        \"\"\"\n",
    "        abs_path = os.path.abspath(f'{mydir}')    #translates user defined path into abssolute path. This prevents issues with user input for backslashes.\n",
    "        dir_list = [f for f in os.listdir(abs_path) if os.path.isdir(os.path.join(abs_path, f))]\n",
    "        \n",
    "        out_list = [[[]  for _ in range(len(extensions))]  for _ in range(len(dir_list))]    #generated [[[ ]]] nested list for populating with extensions per RUN\n",
    "\n",
    "        r=0    #iterator from 0 to len(dir_list), selects empty list index for file depositiona        \n",
    "\n",
    "        ### Loops to itterate over all files in all directories inside user-defined {mydir}. \n",
    "\n",
    "        for ndir in dir_list:    \n",
    "            ndir = f'{abs_path}/{ndir}'    #generates an absolute path to r^th RUN directory. Safety measure just in case. \n",
    "            for f in os.listdir(ndir):    #for all items(files and dirs) in r^th RUN dir\n",
    "                if os.path.isfile(os.path.join(ndir, f)):    #for only the files in r^th RUN dir\n",
    "                    file_path = [f'{ndir}/{f}']    #generate a complete absolute name to the file\n",
    "                    for ext in range(len(extensions)):    #sorting into appropriate list based on user-defined file extensions.\n",
    "                        if os.path.splitext(f)[1] == extensions[ext]:\n",
    "                            out_list[r][ext].extend(file_path)    #save the file into the appropropriate list. \n",
    "                        else:\n",
    "                            continue\n",
    "            r+=1\n",
    "\n",
    "        ### Vince Voelz's sorting for the traj files. orders things from 0..33 properly\n",
    "        chignolin = True\n",
    "        if chignolin == True:\n",
    "\n",
    "            trajs_list = out_list[0]\n",
    "            clone_indices = [ int(os.path.basename(filename).split('_C')[1].replace('.xtc','')) for filename in trajs_list ]\n",
    "            sorted_traj_list = [None]*len(trajs_list)    \n",
    "            for i in range(len(clone_indices)):\n",
    "                sorted_traj_list[clone_indices[i]] = trajs_list[i]\n",
    "            out_list[0] = sorted_traj_list\n",
    "        else:\n",
    "            pass\n",
    "        ### end of dirty sorting. if working with a different dataset, remove in the future\n",
    "\n",
    "        return dir_list, out_list\n",
    "\n",
    "class featurizer(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"Initial function.\n",
    "        Parameters\n",
    "        ----------\n",
    "\n",
    "        \"\"\"\n",
    "        self.title='Featurizer to grab data from xtc files in user specified dir'\n",
    "\n",
    "\n",
    "    def featurizer(self, feat, out_list, out_name, save):\n",
    "        \"\"\"Calculates user-defined feature parameters.\n",
    "\n",
    "        out_list: list(str)\n",
    "            List of paths to trajectory files as indentified in the get_files function\n",
    "        out_name: str\n",
    "            Name of the feature to save npy file to. User preference\n",
    "        \"\"\"\n",
    "        trajs_list = out_list[0]\n",
    "\n",
    "        ### Vince Voelz's sorting for the traj files. orders things from 0..33 properly\n",
    "        chignolin = True\n",
    "        if chignolin == True:\n",
    "            clone_indices = [ int(os.path.basename(filename).split('_C')[1].replace('.xtc','')) for filename in trajs_list ]\n",
    "            sorted_traj_list = [None]*len(trajs_list)    \n",
    "            for i in range(len(clone_indices)):\n",
    "                sorted_traj_list[clone_indices[i]] = trajs_list[i]\n",
    "            print(f'WARNING: You are parsing the CLN001 dataset. Vince Voelzs sorting algorithm applied\\nHere is the list of trajectories being analyzed: {sorted_traj_list}')\n",
    "        else:\n",
    "            pass\n",
    "        trajs_list = sorted_traj_list\n",
    "        ### end of dirty sorting. if working with a different dataset, remove in the future\n",
    "        \n",
    "        feat_data = pyemma.coordinates.load(trajs_list, features=feat)\n",
    "        \n",
    "        if save == True:\n",
    "            np.save(out_name,feat_data)\n",
    "        return feat_data\n",
    "\n",
    "    def score_cv(self, feat_data, dim, lag, number_of_splits, validation_fraction, out_name, save):\n",
    "        \"\"\"Compute a cross-validated VAMP2 score.\n",
    "\n",
    "        We randomly split the list of independent trajectories into\n",
    "        a training and a validation set, compute the VAMP2 score,\n",
    "        and repeat this process several times.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        data : list of numpy.ndarrays\n",
    "            The input data.\n",
    "        dim : int\n",
    "            Number of processes to score; equivalent to the dimension\n",
    "            after projecting the data with VAMP2.\n",
    "        lag : int\n",
    "            Lag time for the VAMP2 scoring.\n",
    "        number_of_splits : int, optional, default=10\n",
    "            How often do we repeat the splitting and score calculation.\n",
    "        validation_fraction : int, optional, default=0.5\n",
    "            Fraction of trajectories which should go into the validation\n",
    "            set during a split.\n",
    "        \"\"\"\n",
    "        nval = int(len(feat_data) * validation_fraction)\n",
    "        scores = np.zeros(number_of_splits)\n",
    "        for n, _ in enumerate(scores):\n",
    "            ival = np.random.choice(len(feat_data), size=nval, replace=False)\n",
    "            valid = [d for i, d in enumerate(feat_data) if i not in ival]\n",
    "            vamp = pyemma.coordinates.vamp(valid, lag=lag, dim=dim, scaling = 'km')\n",
    "            scores[n] = vamp.score([d for i, d in enumerate(feat_data) if i in ival])\n",
    "\n",
    "        if save == True:\n",
    "            np.save(out_name,scores)\n",
    "        return scores\n",
    "\n",
    "### TEMP FUNCTION STORAGE, NEED TO COMMENT ADN CLEAN BEFORE PUBLISHING\n",
    "\n",
    "def tica(feat, lag_time, save, save_dir, save_name, feat_name):\n",
    "    feat_list = np.ndarray.tolist(feat)\n",
    "    tica = pyemma.coordinates.tica(feat_list, dim=8, lag=lag_time, stride=1, kinetic_map=True, commute_map=False)\n",
    "    tica_getoutput = tica.get_output()\n",
    "    if save == True:\n",
    "        tica.save(f'{save_dir}/{feat_name}_tica_raw.h5', overwrite=True)\n",
    "        np.save(f'{save_dir}/{feat_name}_tica_getoutput.npy', tica_getoutput)\n",
    "    return tica_getoutput\n",
    "\n",
    "def k_means(tica_getoutput, n_cluster, max_iter, save, save_dir, save_name, feat_name):\n",
    "    cluster = pyemma.coordinates.cluster_kmeans(tica_getoutput, k=n_cluster, max_iter=max_iter)\n",
    "    cluster_getoutput = cluster.get_output()\n",
    "    cluster_dtrajs = cluster.dtrajs\n",
    "    cluster_centers = cluster.clustercenters\n",
    "    if save == True:\n",
    "        cluster.save(f'{save_dir}/{feat_name}_k{n_cluster}_means_raw.h5', overwrite=True)\n",
    "        np.save(f'{save_dir}/{feat_name}_k{n_cluster}_means_getoutput.npy', cluster_getoutput)\n",
    "        np.save(f'{save_dir}/{feat_name}_k{n_cluster}_means_dtrajs.npy', cluster_dtrajs)\n",
    "        np.save(f'{save_dir}/{feat_name}_k{n_cluster}_means_centers.npy', cluster_centers)\n",
    "    return cluster, cluster_centers\n",
    "\n",
    "def generate_msm(cluster, n_cluster, msm_lag, save, save_dir, save_name, feat_name):\n",
    "    msm = pyemma.msm.estimate_markov_model(cluster.dtrajs, msm_lag)\n",
    "    if save == True:\n",
    "        state_frac_used = msm.active_state_fraction\n",
    "        count_frac_used = msm.active_count_fraction\n",
    "        feat_act_set = msm.active_set\n",
    "        eigvec = msm.eigenvectors_right()\n",
    "\n",
    "        log_save = open(f'{save_dir}/{feat_name}_k{n_cluster}_msm_params.log','w')\n",
    "        log_save.write(f'Fraction of states used: {state_frac_used}.\\nFraction of counts used: {count_frac_used}. \\nMain indices of the active states (members of the largest connected set): {list(feat_act_set)}. \\nStationary distribution across states: {msm.stationary_distribution}. \\nFirst eigenvector is one: {np.allclose(eigvec[:, 0], 1, atol=1e-15)} (min={eigvec[:, 0].min()}, max={eigvec[:, 0].max()})')\n",
    "        log_save.close()\n",
    "        msm.save(f'{save_dir}/{feat_name}_k{n_cluster}_msm_raw.h5',  overwrite=True)\n",
    "    return msm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f31661b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
